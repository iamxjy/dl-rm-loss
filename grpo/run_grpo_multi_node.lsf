#!/bin/bash
# LSF job script for 3 nodes x 8 GPUs (24 processes) using accelerate
# Fill in queue/project paths before submission.

#BSUB -J grpo
#BSUB -q <queue>
#BSUB -nnodes 3
#BSUB -n 3
#BSUB -R "span[ptile=1]"
#BSUB -W 24:00
#BSUB -gpu "num=8:mode=exclusive_process"
#BSUB -env "all"

set -euo pipefail

# Load environment (adjust to your cluster)
module load cuda/12.1 || true
source /path/to/venv/bin/activate

# NCCL networking hints (tune to your interfaces)
export NCCL_DEBUG=warn
# export NCCL_SOCKET_IFNAME=eth0          # change to ib0/enp* as needed
export NCCL_IB_DISABLE=0                # set to 1 if no InfiniBand
export OMP_NUM_THREADS=2

# Master address/port from the first host in the allocation
MASTER_ADDR=$(echo "$LSB_HOSTS" | awk '{print $1}')
MASTER_PORT=29500

CONFIG=grpo/eight_gpu.yaml

# Launch 3 ranks (one per node), each with 8 GPUs
# starts 3 resource sets (one per node), each with 16 CPUs and 8 GPUs
jsrun -n 3 -r 1 -a 1 -c 16 -g 8 bash -c '
  RANK=${JSM_NAMESPACE_RANK:-${JSRUN_RANK:-0}}
  accelerate launch \
    --config_file '"$CONFIG"' \
    --machine_rank $RANK \
    --main_process_ip '"$MASTER_ADDR"' \
    --main_process_port '"$MASTER_PORT"' \
    grpo/grpo_gpu.py \
      --model_name_or_path <model_path> \
      --output_dir <output_dir> \
      --learning_rate 1e-5 \
      --gradient_checkpointing \
      --dtype bfloat16 \
      --max_prompt_length 2048 \
      --max_completion_length 1024 \
      --use_peft \
      --lora_target_modules "q_proj" "v_proj" \
      --reward_model_path <reward_model_path>
'
